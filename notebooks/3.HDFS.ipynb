{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/0c/9c5d5dd254e9e7a32d34777cc6fd33cbeb174744061458b88470aecbd1d6/python_dotenv-0.18.0-py2.py3-none-any.whl\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-0.18.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'HOSTNAME': 'hadoop',\n",
       " 'OLDPWD': '/',\n",
       " 'PWD': '/opt',\n",
       " 'HOME': '/home/hadoop',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'SHLVL': '1',\n",
       " 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/hadoop/sbin',\n",
       " '_': '/usr/bin/nohup',\n",
       " 'JPY_PARENT_PID': '2960',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
       " 'JAVA_HOME': '/usr/lib/jvm/java-1.8.0-openjdk-amd64',\n",
       " 'PDSH_RCMD_TYPE': 'ssh',\n",
       " 'HADOOP_HOME': '/opt/hadoop',\n",
       " 'HADOOP_COMMON_HOME': '/opt/hadoop',\n",
       " 'HADOOP_CONF_DIR': '/opt/hadoop/etc/hadoop',\n",
       " 'HADOOP_HDFS_HOME': '/opt/hadoop',\n",
       " 'HADOOP_MAPRED_HOME': '/opt/hadoop',\n",
       " 'HADOOP_YARN_HOME': '/opt/hadoop'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source /opt/envvars.sh\n",
    "\n",
    "!pip3 install python-dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv -o /opt/envvars.sh\n",
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - Web Interface\n",
    "\n",
    "- Master node\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "- Worker node\n",
    "    - hadoop1\n",
    "        - DataNode: http://localhost:9864\n",
    "    - hadoop2\n",
    "        - DataNode: http://localhost:9865\n",
    "    - hadoop3\n",
    "        - DataNode: http://localhost:9866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "cacheadmin           configure the HDFS cache\n",
      "crypto               configure HDFS encryption zones\n",
      "debug                run a Debug Admin to execute HDFS debug commands\n",
      "dfsadmin             run a DFS admin client\n",
      "dfsrouteradmin       manage Router-based federation\n",
      "ec                   run a HDFS ErasureCoding CLI\n",
      "fsck                 run a DFS filesystem checking utility\n",
      "haadmin              run a DFS HA admin client\n",
      "jmxget               get JMX exported values from NameNode or DataNode.\n",
      "oev                  apply the offline edits viewer to an edits file\n",
      "oiv                  apply the offline fsimage viewer to an fsimage\n",
      "oiv_legacy           apply the offline fsimage viewer to a legacy fsimage\n",
      "storagepolicies      list/get/set/satisfyStoragePolicy block storage policies\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "dfs                  run a filesystem command on the file system\n",
      "envvars              display computed Hadoop environment variables\n",
      "fetchdt              fetch a delegation token from the NameNode\n",
      "getconf              get config values from configuration\n",
      "groups               get the groups which users belong to\n",
      "lsSnapshottableDir   list all snapshottable dirs owned by the current user\n",
      "snapshotDiff         diff two snapshots of a directory or diff the current\n",
      "                     directory contents with a snapshot\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "balancer             run a cluster balancing utility\n",
      "datanode             run a DFS datanode\n",
      "dfsrouter            run the DFS router\n",
      "diskbalancer         Distributes data evenly among disks on a given node\n",
      "journalnode          run the DFS journalnode\n",
      "mover                run a utility to move block replicas across storage types\n",
      "namenode             run the DFS namenode\n",
      "nfs3                 run an NFS version 3 gateway\n",
      "portmap              run a portmap service\n",
      "secondarynamenode    run the DFS secondary namenode\n",
      "sps                  run external storagepolicysatisfier\n",
      "zkfc                 run the ZK Failover Controller daemon\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem Basic Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "Download books from Gutenberg project (http://www.gutenberg.org/)\n",
    "\n",
    "- Moby Dick; Or, The Whale by Herman Melville\n",
    "- Pride and Prejudice by Jane Austen\n",
    "- Dracula by Bram Stoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dracula.txt\n",
      "mobydick.txt\n",
      "prideandprejudice.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "wget -qc http://www.gutenberg.org/files/2701/2701-0.txt -O mobydick.txt\n",
    "wget -qc http://www.gutenberg.org/files/1342/1342-0.txt -O prideandprejudice.txt\n",
    "wget -qc http://www.gutenberg.org/cache/epub/345/pg345.txt -O dracula.txt\n",
    "\n",
    "ls /opt/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "# create gutenberg folder in HDFS\n",
    "# hdfs dfs -mkdir /user/hadoop/gutenberg\n",
    "\n",
    "# copy books to HDFS\n",
    "# hdfs dfs -put * /user/hadoop/gutenberg\n",
    "# hdfs dfs -copyFromLocal * /user/hadoop/gutenberg\n",
    "\n",
    "# list files in HDFS\n",
    "# hdfs dfs -ls /user/hadoop/gutenberg\n",
    "\n",
    "# show first KB of file\n",
    "# hdfs dfs -head /user/hadoop/gutenberg/mobydick.txt\n",
    "\n",
    "# show last KB of file\n",
    "# hdfs dfs -tail /user/hadoop/gutenberg/prideandprejudice.txt\n",
    "\n",
    "# show whole file - CAREFUL\n",
    "# hdfs dfs -cat /user/hadoop/gutenberg/dracula.txt\n",
    "\n",
    "# append file contents to a file in HDFS\n",
    "# hdfs dfs -appendToFile mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/allbooks.txt\n",
    "\n",
    "# copy allbooks.txt (in HDFS) to gutenberg directory (in HDFS)\n",
    "# hdfs dfs -cp allbooks.txt /user/hadoop/gutenberg\n",
    "# hdfs dfs -ls -h -R\n",
    "\n",
    "# retrieve allbooks.txt from HDFS\n",
    "# hdfs dfs -get allbooks.txt .\n",
    "# hdfs dfs -copyToLocal /user/hadoop/allbooks.txt .\n",
    "\n",
    "# remove file\n",
    "# hdfs dfs -rm allbooks.txt\n",
    "# hdfs dfs -rm /user/hadoop/allbooks.txt\n",
    "\n",
    "# mv file (also used for renaming)\n",
    "# hdfs dfs -mv gutenberg/allbooks.txt gutenberg/books.txt\n",
    "\n",
    "# print statistics on folder\n",
    "# printf \"name\\ttype\\tsize\\treps\\n\"\n",
    "# hdfs dfs -stat \"%n %F %b %r\" /user/hadoop/gutenberg/*\n",
    "\n",
    "# getmerge\n",
    "# hdfs dfs -getmerge /user/hadoop/gutenberg mergebooks.txt\n",
    "\n",
    "# remove directory and files (-R recursive)\n",
    "# hdfs dfs -rm -R /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilization in a MapReduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/gutenberg\n",
    "hdfs dfs -put mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-27 19:56:55,763 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-06-27 19:56:55,894 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-06-27 19:56:56,088 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1624820343693_0001\n",
      "2021-06-27 19:56:56,375 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2021-06-27 19:56:56,494 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2021-06-27 19:56:56,645 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1624820343693_0001\n",
      "2021-06-27 19:56:56,647 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-06-27 19:56:56,794 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-06-27 19:56:56,794 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-06-27 19:56:57,176 INFO impl.YarnClientImpl: Submitted application application_1624820343693_0001\n",
      "2021-06-27 19:56:57,207 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1624820343693_0001/\n",
      "2021-06-27 19:56:57,207 INFO mapreduce.Job: Running job: job_1624820343693_0001\n",
      "2021-06-27 19:57:03,320 INFO mapreduce.Job: Job job_1624820343693_0001 running in uber mode : false\n",
      "2021-06-27 19:57:03,321 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-06-27 19:57:08,372 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2021-06-27 19:57:09,385 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-06-27 19:57:13,422 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-06-27 19:57:13,442 INFO mapreduce.Job: Job job_1624820343693_0001 completed successfully\n",
      "2021-06-27 19:57:13,544 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=709870\n",
      "\t\tFILE: Number of bytes written=2359637\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2076240\n",
      "\t\tHDFS: Number of bytes written=459660\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=20740\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3894\n",
      "\t\tTotal time spent by all map tasks (ms)=10370\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1947\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10370\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1947\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2654720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=498432\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=36896\n",
      "\t\tMap output records=340606\n",
      "\t\tMap output bytes=3323581\n",
      "\t\tMap output materialized bytes=709882\n",
      "\t\tInput split bytes=362\n",
      "\t\tCombine input records=340606\n",
      "\t\tCombine output records=47214\n",
      "\t\tReduce input groups=40654\n",
      "\t\tReduce shuffle bytes=709882\n",
      "\t\tReduce input records=47214\n",
      "\t\tReduce output records=40654\n",
      "\t\tSpilled Records=94428\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=321\n",
      "\t\tCPU time spent (ms)=5920\n",
      "\t\tPhysical memory (bytes) snapshot=1042706432\n",
      "\t\tVirtual memory (bytes) snapshot=7773691904\n",
      "\t\tTotal committed heap usage (bytes)=673710080\n",
      "\t\tPeak Map Physical memory (bytes)=293412864\n",
      "\t\tPeak Map Virtual memory (bytes)=1940336640\n",
      "\t\tPeak Reduce Physical memory (bytes)=182349824\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1953832960\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2075878\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=459660\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.2.jar wordcount \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2021-06-27 19:57 /user/hadoop/gutenberg-output/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop     459660 2021-06-27 19:57 /user/hadoop/gutenberg-output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# list output folder contents\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Defects,\"\t2\n",
      "\"Information\t2\n",
      "\"Plain\t4\n",
      "\"Project\t10\n",
      "\"Right\t2\n",
      "#1342]\t1\n",
      "#2701]\t1\n",
      "$20,000,000!\t1\n",
      "$5,000)\t2\n",
      "$7,000,000.\t1\n",
      "&\t2\n",
      "'AS-IS',\t2\n",
      "(\"the\t2\n",
      "($1\t2\n",
      "(1775)\t1\n",
      "(801)\t2\n",
      "(Ahab���s)\t1\n",
      "(Albatross)\t1\n",
      "(American)\t1\n",
      "(Bunger,\t1\n",
      "(Fife)\t1\n",
      "(Greenland\t1\n",
      "(I\t1\n",
      "(Lady\t1\n",
      "(Pig-fish\t1\n",
      "(Pull,\t1\n",
      "(Sperm\t3\n",
      "(Spring,\t1\n",
      "(Steelkilt)\t1\n",
      "(Steelkilt���s)\t1\n",
      "(Strong,\t1\n",
      "(Supplied\t3\n",
      "(Terra\t1\n",
      "(_A\t1\n",
      "(_Advancing_.)\t1\n",
      "(_Ahab\t3\n",
      "(_Algerine\t1\n",
      "(_As\t1\n",
      "(_Ascending,\t1\n",
      "(_Aside_)\t1\n",
      "(_Aside_.)\t1\n",
      "(_Black\t1\n",
      "(_Carpenter\t1\n",
      "(_Dancing_)\t1\n",
      "(_Duodecimo_),\t3\n",
      "(_Duodecimo_).\t1\n",
      "(_During\t1\n",
      "(_Enter\t1\n",
      "(_Fin-Back_).���Under\t1\n",
      "(_Folio_)\t1\n",
      "(_Folio_),\t6\n",
      "(_Foresail\t1\n",
      "(_Grampus_).���Though\t1\n",
      "(_Hump\t1\n",
      "(_Huzza\t1\n",
      "(_Killer_).���Of\t1\n",
      "(_Leaps\t1\n",
      "(_Mealy-mouthed\t1\n",
      "(_Narwhale_),\t1\n",
      "(_Nudging_.)\t1\n",
      "(_Octavo_),\t6\n",
      "(_Octavo_).\t1\n",
      "(_Quietly\t1\n",
      "(_Razor\t1\n",
      "(_Reclining\t2\n",
      "(_Reclining_.)\t1\n",
      "(_Right\t1\n",
      "(_Sings,\t1\n",
      "(_Sperm\t1\n",
      "(_Stubb\t1\n",
      "(_Sulky\t1\n",
      "(_Sulphur\t1\n",
      "(_The\t1\n",
      "(_They\t2\n",
      "(_Thrasher_).���This\t1\n",
      "(_advancing_).\t1\n",
      "(_aside_).\t1\n",
      "(_cabinet_\t1\n",
      "(_grimly_).\t1\n",
      "(_i.e._,\t1\n",
      "(_meeting\t1\n",
      "(_or\t1\n",
      "(_resuming\t1\n",
      "(_shrinking\t1\n",
      "(_snee"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# show head\n",
    "hdfs dfs -head /user/hadoop/gutenberg-output/part-r-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Defects,\"\t2\n",
      "\"Information\t2\n",
      "\"Plain\t4\n",
      "\"Project\t10\n",
      "\"Right\t2\n",
      "#1342]\t1\n",
      "#2701]\t1\n",
      "$20,000,000!\t1\n",
      "$5,000)\t2\n",
      "$7,000,000.\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /tmp\n",
    "\n",
    "# copy HDFS file to local filesystem\n",
    "hdfs dfs -get /user/hadoop/gutenberg-output/part-r-00000 gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/hadoop/gutenberg-output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# remove folder on HDFS\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-27 20:14:54,869 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-06-27 20:14:55,030 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-06-27 20:14:55,241 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1624820343693_0002\n",
      "2021-06-27 20:14:55,512 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2021-06-27 20:14:55,663 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2021-06-27 20:14:55,825 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1624820343693_0002\n",
      "2021-06-27 20:14:55,826 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-06-27 20:14:55,985 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-06-27 20:14:55,985 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-06-27 20:14:56,039 INFO impl.YarnClientImpl: Submitted application application_1624820343693_0002\n",
      "2021-06-27 20:14:56,080 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1624820343693_0002/\n",
      "2021-06-27 20:14:56,081 INFO mapreduce.Job: Running job: job_1624820343693_0002\n",
      "2021-06-27 20:15:01,188 INFO mapreduce.Job: Job job_1624820343693_0002 running in uber mode : false\n",
      "2021-06-27 20:15:01,189 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-06-27 20:15:06,262 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-06-27 20:15:10,289 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2021-06-27 20:15:11,294 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-06-27 20:15:12,304 INFO mapreduce.Job: Job job_1624820343693_0002 completed successfully\n",
      "2021-06-27 20:15:12,376 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=709876\n",
      "\t\tFILE: Number of bytes written=2594658\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2076240\n",
      "\t\tHDFS: Number of bytes written=459660\n",
      "\t\tHDFS: Number of read operations=19\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tOther local map tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18768\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10810\n",
      "\t\tTotal time spent by all map tasks (ms)=9384\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5405\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9384\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5405\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2402304\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1383680\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=36896\n",
      "\t\tMap output records=340606\n",
      "\t\tMap output bytes=3323581\n",
      "\t\tMap output materialized bytes=709900\n",
      "\t\tInput split bytes=362\n",
      "\t\tCombine input records=340606\n",
      "\t\tCombine output records=47214\n",
      "\t\tReduce input groups=40654\n",
      "\t\tReduce shuffle bytes=709900\n",
      "\t\tReduce input records=47214\n",
      "\t\tReduce output records=40654\n",
      "\t\tSpilled Records=94428\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=326\n",
      "\t\tCPU time spent (ms)=7680\n",
      "\t\tPhysical memory (bytes) snapshot=1225609216\n",
      "\t\tVirtual memory (bytes) snapshot=9723449344\n",
      "\t\tTotal committed heap usage (bytes)=765460480\n",
      "\t\tPeak Map Physical memory (bytes)=288641024\n",
      "\t\tPeak Map Virtual memory (bytes)=1942589440\n",
      "\t\tPeak Reduce Physical memory (bytes)=185970688\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1949388800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2075878\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=459660\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application with 2 reducers\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.2.jar wordcount \\\n",
    "-Dmapreduce.job.reduces=2 \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2021-06-27 20:15 /user/hadoop/gutenberg-output/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop     230097 2021-06-27 20:15 /user/hadoop/gutenberg-output/part-r-00000\n",
      "-rw-r--r--   2 hadoop hadoop     229563 2021-06-27 20:15 /user/hadoop/gutenberg-output/part-r-00001\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# list output folder contents\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Project\t10\n",
      "$20,000,000!\t1\n",
      "$7,000,000.\t1\n",
      "'AS-IS',\t2\n",
      "(\"the\t2\n",
      "($1\t2\n",
      "(1775)\t1\n",
      "(Ahab���s)\t1\n",
      "(American)\t1\n",
      "(Bunger,\t1\n",
      "Deleted /user/hadoop/gutenberg-output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /tmp\n",
    "\n",
    "# copy HDFS file to local filesystem\n",
    "hdfs dfs -getmerge /user/hadoop/gutenberg-output gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt\n",
    "\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Commands\n",
    "\n",
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify HDFS cluster status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rack: /default-rack\n",
      "   172.17.0.3:9866 (hadoop1)\n",
      "   172.17.0.4:9866 (hadoop2)\n",
      "   172.17.0.5:9866 (hadoop3)\n",
      "\n",
      "\n",
      "========================================\n",
      "\n",
      "Configured Capacity: 374878420992 (349.13 GB)\n",
      "Present Capacity: 49093227958 (45.72 GB)\n",
      "DFS Remaining: 49086664704 (45.72 GB)\n",
      "DFS Used: 6563254 (6.26 MB)\n",
      "DFS Used%: 0.01%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 2030881 (1.94 MB)\n",
      "Non DFS Used: 102203724511 (95.18 GB)\n",
      "DFS Remaining: 16362074112 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:15:41 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:51:56 GMT 2021\n",
      "Num of Blocks: 7\n",
      "\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 1421994 (1.36 MB)\n",
      "Non DFS Used: 102204083542 (95.18 GB)\n",
      "DFS Remaining: 16362323968 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:15:41 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:56:17 GMT 2021\n",
      "Num of Blocks: 6\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 3110379 (2.97 MB)\n",
      "Non DFS Used: 102202452501 (95.18 GB)\n",
      "DFS Remaining: 16362266624 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:15:42 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:49:09 GMT 2021\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# print topology\n",
    "hdfs dfsadmin -printTopology\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" |tr \" \" \"=\"\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSCK started by hadoop (auth:SIMPLE) from /172.17.0.2 for path /user/hadoop/gutenberg at Sun Jun 27 20:17:35 GMT 2021\n",
      "\n",
      "/user/hadoop/gutenberg <dir>\n",
      "/user/hadoop/gutenberg/dracula.txt 0 bytes, replicated: replication=2, 0 block(s):  OK\n",
      "\n",
      "/user/hadoop/gutenberg/mobydick.txt 1276233 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-358313605-172.17.0.2-1624820476678:blk_1073741829_1005 len=1276233 Live_repl=2  [DatanodeInfoWithStorage[172.17.0.3:9866,DS-f29f3cb9-1fe3-475e-9530-1cd7805d15c5,DISK], DatanodeInfoWithStorage[172.17.0.4:9866,DS-97d010e6-a95e-40ff-b3b6-ac2cef33bb41,DISK]]\n",
      "\n",
      "/user/hadoop/gutenberg/prideandprejudice.txt 799645 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-358313605-172.17.0.2-1624820476678:blk_1073741830_1006 len=799645 Live_repl=2  [DatanodeInfoWithStorage[172.17.0.4:9866,DS-97d010e6-a95e-40ff-b3b6-ac2cef33bb41,DISK], DatanodeInfoWithStorage[172.17.0.3:9866,DS-f29f3cb9-1fe3-475e-9530-1cd7805d15c5,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t3\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t1\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t2075878 B\n",
      " Total files:\t3\n",
      " Total blocks (validated):\t2 (avg. block size 1037939 B)\n",
      " Minimally replicated blocks:\t2 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t2.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      "FSCK ended at Sun Jun 27 20:17:35 GMT 2021 in 1 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/hadoop/gutenberg' is HEALTHY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://hadoop:9870/fsck?ugi=hadoop&files=1&blocks=1&locations=1&path=%2Fuser%2Fhadoop%2Fgutenberg\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# list folder block location\n",
    "#hdfs fsck /user/hadoop/gutenberg -files -blocks -locations\n",
    "\n",
    "# change replication factor of all files in directory to 3\n",
    "#hdfs dfs -setrep 3 /user/hadoop/gutenberg\n",
    "\n",
    "# list folder block location\n",
    "#hdfs fsck /user/hadoop/gutenberg -files -blocks -locations\n",
    "\n",
    "# change replication factor back to 2\n",
    "#hdfs dfs -setrep 2 /user/hadoop/gutenberg\n",
    "\n",
    "# list folder block location\n",
    "hdfs fsck /user/hadoop/gutenberg -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomission nodes\n",
    "\n",
    "- dfs.hosts.exclude in hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh nodes successful\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Decomissioning hadoop1\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "hadoop1\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 249918947328 (232.76 GB)\n",
      "Present Capacity: 32725126064 (30.48 GB)\n",
      "DFS Remaining: 32718848000 (30.47 GB)\n",
      "DFS Used: 6278064 (5.99 MB)\n",
      "DFS Used%: 0.02%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Decommissioned\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 2604425 (2.48 MB)\n",
      "Non DFS Used: 102205829751 (95.19 GB)\n",
      "DFS Remaining: 16359395328 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:18:23 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:51:56 GMT 2021\n",
      "Num of Blocks: 7\n",
      "\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 3137125 (2.99 MB)\n",
      "Non DFS Used: 102205297051 (95.19 GB)\n",
      "DFS Remaining: 16359395328 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:18:23 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:56:17 GMT 2021\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 3140939 (3.00 MB)\n",
      "Non DFS Used: 102205235893 (95.19 GB)\n",
      "DFS Remaining: 16359452672 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:18:21 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:49:09 GMT 2021\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# report HDFS status\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh nodes successful\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Recomission all nodes\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 374878420992 (349.13 GB)\n",
      "Present Capacity: 49086867769 (45.72 GB)\n",
      "DFS Remaining: 49077985280 (45.71 GB)\n",
      "DFS Used: 8882489 (8.47 MB)\n",
      "DFS Used%: 0.02%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 2604425 (2.48 MB)\n",
      "Non DFS Used: 102205850231 (95.19 GB)\n",
      "DFS Remaining: 16359374848 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:18:50 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:51:56 GMT 2021\n",
      "Num of Blocks: 7\n",
      "\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 3137125 (2.99 MB)\n",
      "Non DFS Used: 102205317531 (95.19 GB)\n",
      "DFS Remaining: 16359374848 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:18:50 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:56:17 GMT 2021\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 3140939 (3.00 MB)\n",
      "Non DFS Used: 102205452981 (95.19 GB)\n",
      "DFS Remaining: 16359235584 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:18:48 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:49:09 GMT 2021\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# report HDFS status\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling datanode failures\n",
    "\n",
    "- timeouts defined in hdfs-site.xml \n",
    "    - dfs.namenode.heartbeat.recheck-interval = 10000 (10 seconds)\n",
    "    - dfs.heartbeat.interval = 3 seconds\n",
    "- timeout = 2 x recheck-interval + 10 x heartbeat.interval\n",
    "    - timeout = 50 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "3s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# get dfs.namenode.heartbeat.recheck-interval\n",
    "hdfs getconf -confKey dfs.namenode.heartbeat.recheck-interval\n",
    "\n",
    "# get dfs.heartbeat.interval\n",
    "hdfs getconf -confKey dfs.heartbeat.interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# simulate node fault\n",
    "ssh hadoop1 'kill -9 $(cat /tmp/hadoop-hadoop-datanode.pid)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 249918947328 (232.76 GB)\n",
      "Present Capacity: 32719588683 (30.47 GB)\n",
      "DFS Remaining: 32713277440 (30.47 GB)\n",
      "DFS Used: 6311243 (6.02 MB)\n",
      "DFS Used%: 0.02%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (2):\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 3170304 (3.02 MB)\n",
      "Non DFS Used: 102208020480 (95.19 GB)\n",
      "DFS Remaining: 16356638720 (15.23 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:21:17 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:56:17 GMT 2021\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 3140939 (3.00 MB)\n",
      "Non DFS Used: 102208049845 (95.19 GB)\n",
      "DFS Remaining: 16356638720 (15.23 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:21:18 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:49:09 GMT 2021\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 2604425 (2.48 MB)\n",
      "Non DFS Used: 102203736695 (95.18 GB)\n",
      "DFS Remaining: 16361488384 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.09%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Sun Jun 27 20:19:11 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:51:56 GMT 2021\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Restart nodemanager\n",
    "ssh hadoop1 /opt/hadoop/bin/hdfs --daemon start datanode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 374878420992 (349.13 GB)\n",
      "Present Capacity: 49103261003 (45.73 GB)\n",
      "DFS Remaining: 49094324224 (45.72 GB)\n",
      "DFS Used: 8936779 (8.52 MB)\n",
      "DFS Used%: 0.02%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 7\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 2625536 (2.50 MB)\n",
      "Non DFS Used: 102200414208 (95.18 GB)\n",
      "DFS Remaining: 16364789760 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.10%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:22:10 GMT 2021\n",
      "Last Block Report: Sun Jun 27 20:22:10 GMT 2021\n",
      "Num of Blocks: 7\n",
      "\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 3170304 (3.02 MB)\n",
      "Non DFS Used: 102199873536 (95.18 GB)\n",
      "DFS Remaining: 16364785664 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.10%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:22:11 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:56:17 GMT 2021\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 124959473664 (116.38 GB)\n",
      "DFS Used: 3140939 (3.00 MB)\n",
      "Non DFS Used: 102199939765 (95.18 GB)\n",
      "DFS Remaining: 16364748800 (15.24 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 13.10%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Jun 27 20:22:12 GMT 2021\n",
      "Last Block Report: Sun Jun 27 19:49:09 GMT 2021\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
