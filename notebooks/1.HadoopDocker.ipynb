{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop - multi-node cluster setup \n",
    "![Hadoop](https://hadoop.apache.org/elephant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hadoop base image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker container\n",
    "\n",
    "- Ubuntu 18.04 (https://ubuntu.com/)\n",
    "- Docker (https://www.docker.com/)\n",
    "    - container based virtualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4084bf3511e3c8eef467f5afd224267d1e49e0ec8ee5f9124f518052b6abc5f5\n",
      "CONTAINER ID   IMAGE                                                    COMMAND                  CREATED        STATUS                  PORTS                                                                                  NAMES\n",
      "4084bf3511e3   ubuntu:18.04                                             \"bash\"                   1 second ago   Up Less than a second                                                                                          hadoopimg\n",
      "61b6274d5344   bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8     \"/entrypoint.sh /run…\"   3 days ago     Up 4 hours (healthy)    8188/tcp                                                                               historyserver\n",
      "0d86bc20719c   bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8       \"/entrypoint.sh /run…\"   3 days ago     Up 4 hours (healthy)    8042/tcp                                                                               nodemanager\n",
      "7437e5470c0e   bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8          \"/entrypoint.sh /run…\"   3 days ago     Up 4 hours (healthy)    0.0.0.0:9000->9000/tcp, :::9000->9000/tcp, 0.0.0.0:9870->9870/tcp, :::9870->9870/tcp   namenode\n",
      "f1ad65fd10d7   bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8          \"/entrypoint.sh /run…\"   3 days ago     Up 4 hours (healthy)    9864/tcp                                                                               datanode\n",
      "e512f9001397   bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8   \"/entrypoint.sh /run…\"   3 days ago     Up 4 hours (healthy)    8088/tcp                                                                               resourcemanager\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "docker run -d -t --rm --name hadoopimg -h hadoopimg ubuntu:18.04\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "- Java 8 (OpenJDK) - https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\n",
    "- Other packages: ssh pdsh wget apt-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoopimg\n",
    "\n",
    "# Update package list\n",
    "apt -qq update > /install.log 2>&1\n",
    "\n",
    "# Install Hadoop dependencies\n",
    "apt -qq -f -y install openjdk-8-jdk ssh pdsh >> /install.log 2>&1\n",
    "\n",
    "# Install other dependencies\n",
    "apt -qq -f -y install vim wget apt-utils python3 python3-pip \\\n",
    "    ipython3 less unzip sudo net-tools >> /install.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Hadoop\n",
    "\n",
    "- http://hadoop.apache.org/\n",
    "- Version 3.2.1\n",
    "- Base directory: /opt\n",
    "- User/group: hadoop/hadoop\n",
    "- Package with binaries (version 3.2.1): https://hadoop.apache.org/releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoopimg\n",
    "\n",
    "# Enable rwx for all on /opt\n",
    "chmod 777 /opt\n",
    "\n",
    "# Create user/group hadoop\n",
    "useradd -m -U -s /bin/bash hadoop\n",
    "\n",
    "# Enable sudo for hadoop\n",
    "sed -i \"\\$ahadoop  ALL=(ALL) NOPASSWD:ALL\" /etc/sudoers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download package\n",
    "cd ../pkgs\n",
    "wget -q -c https://downloads.apache.org/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz\n",
    "\n",
    "# Copy installation package to container\n",
    "docker cp hadoop-3.2.2.tar.gz hadoopimg:/opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "rm /opt/hadoop\n",
    "\n",
    "# Modify user/group permissions and unpack file\n",
    "sudo chown hadoop:hadoop /opt/hadoop-3.2.2.tar.gz\n",
    "tar -zxf /opt/hadoop-3.2.2.tar.gz -C /opt\n",
    "rm /opt/hadoop-3.2.2.tar.gz\n",
    "\n",
    "# Create link\n",
    "ln -s /opt/hadoop-3.2.2 /opt/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment variables\n",
    "\n",
    "- Create file /opt/envvars.sh with environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
      "export PDSH_RCMD_TYPE=ssh\n",
      "\n",
      "export HADOOP_HOME=/opt/hadoop\n",
      "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
      "export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
      "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
      "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
      "export HADOOP_YARN_HOME=${HADOOP_HOME}\n",
      "\n",
      "export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/envvars.sh << EOF\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
    "export PDSH_RCMD_TYPE=ssh\n",
    "\n",
    "export HADOOP_HOME=/opt/hadoop\n",
    "export HADOOP_COMMON_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_CONF_DIR=\\${HADOOP_HOME}/etc/hadoop\n",
    "export HADOOP_HDFS_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_MAPRED_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_YARN_HOME=\\${HADOOP_HOME}\n",
    "\n",
    "export PATH=\\${PATH}:\\${HADOOP_HOME}/bin:\\${HADOOP_HOME}/sbin     \n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure passwordless ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    StrictHostKeyChecking no\n",
      "    UserKnownHostsFile /dev/null\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "# Disable host key checking\n",
    "sudo tee -a /etc/ssh/ssh_config << EOF\n",
    "    StrictHostKeyChecking no\n",
    "    UserKnownHostsFile /dev/null\n",
    "EOF\n",
    "\n",
    "# Create ssh key\n",
    "ssh-keygen -q -t rsa -P \"\" -f ~/.ssh/id_rsa\n",
    "\n",
    "# Copy public key to authorized_keys file\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop configuration files\n",
    "\n",
    "- Hadoop configuration files location: \\$HADOOP\\_HOME\\/etc\\/hadoop\n",
    "- All cluster nodes contain the same files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hadoop-env.sh\n",
    "\n",
    "- Definition of environment variables used by Hadoop processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat >> /opt/hadoop/etc/hadoop/hadoop-env.sh << EOF\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### core-site.xml\n",
    "\n",
    "- Hadoop main configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/core-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/core-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://hadoop:9000</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hadoop.groups</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hadoop.hosts</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hdfs-site.xml\n",
    "\n",
    "- HDFS configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/hdfs-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>/opt/hadoop/data/nameNode</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>/opt/hadoop/data/dataNode</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>2</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.blocksize</name>\n",
    "    <value>33554432</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.hosts.exclude</name>\n",
    "    <value>/opt/hadoop/etc/hadoop/dfs.exclude</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.namenode.heartbeat.recheck-interval</name>\n",
    "    <value>10000</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### yarn-site.xml\n",
    "\n",
    "- YARN configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/yarn-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>hadoop</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "    <value>1536</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "    <value>1536</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "    <value>128</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.timeline-service.enabled</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.timeline-service.hostname</name>\n",
    "    <value>hadoop</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.system-metrics-publisher.enabled</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.log-aggregation-enable</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nm.liveness-monitor.expiry-interval-ms</name>\n",
    "    <value>10000</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mapred-site.xml\n",
    "\n",
    "- MapReduce configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/mapred-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.application.classpath</name>\n",
    "    <value>/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/mapreduce/lib/*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.app.mapreduce.am.resource.mb</name>\n",
    "    <value>512</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.map.memory.mb</name>\n",
    "    <value>256</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.reduce.memory.mb</name>\n",
    "    <value>256</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### workers\n",
    "\n",
    "- List of worker nodes (NodeManager and DataNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/workers << EOF\n",
    "hadoop1\n",
    "hadoop2\n",
    "hadoop3\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:7390b5b85b05c1226578f06e357184cbb7b72b1eb6e9697166adcc6fa80969a6\n",
      "hadoopimg\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create hadoopimg image based on hadoop container\n",
    "docker commit hadoopimg hadoopimg\n",
    "\n",
    "# Stop base container\n",
    "docker stop hadoopimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE       COMMAND   CREATED         STATUS         PORTS                                                                                                                                                                                                                                                                                                             NAMES\n",
      "fb54f623a7f6   hadoopimg   \"bash\"    4 minutes ago   Up 4 minutes   0.0.0.0:8044->8042/tcp, :::8044->8042/tcp, 0.0.0.0:9866->9864/tcp, :::9866->9864/tcp                                                                                                                                                                                                                              hadoop3\n",
      "a2c64460f6c9   hadoopimg   \"bash\"    4 minutes ago   Up 4 minutes   0.0.0.0:8043->8042/tcp, :::8043->8042/tcp, 0.0.0.0:9865->9864/tcp, :::9865->9864/tcp                                                                                                                                                                                                                              hadoop2\n",
      "e3ccef4919c8   hadoopimg   \"bash\"    4 minutes ago   Up 4 minutes   0.0.0.0:8042->8042/tcp, :::8042->8042/tcp, 0.0.0.0:9864->9864/tcp, :::9864->9864/tcp                                                                                                                                                                                                                              hadoop1\n",
      "a6c69353170e   hadoopimg   \"bash\"    4 minutes ago   Up 4 minutes   0.0.0.0:4040->4040/tcp, :::4040->4040/tcp, 0.0.0.0:8080->8080/tcp, :::8080->8080/tcp, 0.0.0.0:8088->8088/tcp, :::8088->8088/tcp, 0.0.0.0:8188->8188/tcp, :::8188->8188/tcp, 0.0.0.0:9868->9868/tcp, :::9868->9868/tcp, 0.0.0.0:9870->9870/tcp, :::9870->9870/tcp, 0.0.0.0:19888->19888/tcp, :::19888->19888/tcp   hadoop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/hadoop\" is already in use by container \"a6c69353170e9a0d158fc0b005bcc2c688ab1c63e0802f7e681fb041e009b103\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n",
      "docker: Error response from daemon: Conflict. The container name \"/hadoop1\" is already in use by container \"e3ccef4919c84433a4f59ea36c39b7173ed3311e71ed18b3a48c508e78fca0f9\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n",
      "docker: Error response from daemon: Conflict. The container name \"/hadoop2\" is already in use by container \"a2c64460f6c92e98dfbfe1c078baed5e0fe4c12edb8e01191556ec00bfb3684e\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n",
      "docker: Error response from daemon: Conflict. The container name \"/hadoop3\" is already in use by container \"fb54f623a7f697bcf45d7da33fd92a4f182ffc21caac7b3948f142ecc86482e9\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# MASTER\n",
    "\n",
    "# Ports\n",
    "# 9870 - Namenode\n",
    "# 9868 - Secondary Namenode\n",
    "# 8088 - ResourceManager\n",
    "# 19888 - MapReduce Job History\n",
    "# 8188 - Timeline Service\n",
    "# 4040 - Spark Application UI\n",
    "# 8080 - Jupyter\n",
    "\n",
    "cd ..\n",
    "docker run -d -t --memory 4g --memory-swap 4g --rm --name hadoop -h hadoop -u hadoop \\\n",
    "    -v \"$(pwd)\"/pkgs:/opt/pkgs -v \"$(pwd)\"/notebooks:/opt/notebooks -v \"$(pwd)\"/datasets:/opt/datasets \\\n",
    "    -p 9870:9870 -p 9868:9868 -p 8088:8088 -p 19888:19888 -p 8188:8188 -p 4040:4040 -p 8080:8080 hadoopimg\n",
    "\n",
    "# WORKERS\n",
    "\n",
    "# Ports\n",
    "# 9864 - DataNode WebUI\n",
    "# 8042 - NodeManager WebUI\n",
    "\n",
    "# Hadoop1\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop1 -h hadoop1 -u hadoop \\\n",
    "    -p 9864:9864 -p 8042:8042 hadoopimg\n",
    "# Hadoop2\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop2 -h hadoop2 -u hadoop \\\n",
    "    -p 9865:9864 -p 8043:8042  hadoopimg\n",
    "# Hadoop3\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop3 -h hadoop3 -u hadoop \\\n",
    "    -p 9866:9864 -p 8044:8042  hadoopimg\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure hosts file on all nodes\n",
    "\n",
    "- /etc/hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE       COMMAND   CREATED         STATUS         PORTS                                                                                                                                                                                                                                                                                                             NAMES\n",
      "fb54f623a7f6   hadoopimg   \"bash\"    4 minutes ago   Up 4 minutes   0.0.0.0:8044->8042/tcp, :::8044->8042/tcp, 0.0.0.0:9866->9864/tcp, :::9866->9864/tcp                                                                                                                                                                                                                              hadoop3\n",
      "a2c64460f6c9   hadoopimg   \"bash\"    4 minutes ago   Up 4 minutes   0.0.0.0:8043->8042/tcp, :::8043->8042/tcp, 0.0.0.0:9865->9864/tcp, :::9865->9864/tcp                                                                                                                                                                                                                              hadoop2\n",
      "e3ccef4919c8   hadoopimg   \"bash\"    4 minutes ago   Up 4 minutes   0.0.0.0:8042->8042/tcp, :::8042->8042/tcp, 0.0.0.0:9864->9864/tcp, :::9864->9864/tcp                                                                                                                                                                                                                              hadoop1\n",
      "a6c69353170e   hadoopimg   \"bash\"    4 minutes ago   Up 4 minutes   0.0.0.0:4040->4040/tcp, :::4040->4040/tcp, 0.0.0.0:8080->8080/tcp, :::8080->8080/tcp, 0.0.0.0:8088->8088/tcp, :::8088->8088/tcp, 0.0.0.0:8188->8188/tcp, :::8188->8188/tcp, 0.0.0.0:9868->9868/tcp, :::9868->9868/tcp, 0.0.0.0:9870->9870/tcp, :::9870->9870/tcp, 0.0.0.0:19888->19888/tcp, :::19888->19888/tcp   hadoop\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.17.0.2 hadoop\n",
      "172.17.0.3 hadoop1\n",
      "172.17.0.4 hadoop2\n",
      "172.17.0.5 hadoop3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Get IPs\n",
    "M=$(docker inspect hadoop | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H1=$(docker inspect hadoop1 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H2=$(docker inspect hadoop2 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H3=$(docker inspect hadoop3 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "\n",
    "# Create hosts file\n",
    "cat > hosts << EOF  \n",
    "$M hadoop\n",
    "$H1 hadoop1\n",
    "$H2 hadoop2\n",
    "$H3 hadoop3\n",
    "EOF\n",
    "\n",
    "cat hosts\n",
    "\n",
    "# Copy to all nodes\n",
    "docker exec -i -u root hadoop sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop1 sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop2 sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop3 sh -c 'cat >> /etc/hosts' < hosts\n",
    "\n",
    "# Remove local file\n",
    "rm hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ssh server on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop1\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop2\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop3\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3\n",
    "do\n",
    "    echo $HOST\n",
    "    docker exec -u root $HOST service ssh restart\n",
    "    docker exec -u root $HOST service ssh status\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format HDFS on Namenode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-27 19:01:15,928 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = hadoop/172.17.0.2\n",
      "STARTUP_MSG:   args = [-format, -force, -nonInteractive]\n",
      "STARTUP_MSG:   version = 3.2.2\n",
      "STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.10.6.Final.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.9.10.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.9.10.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.9.10.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.19.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.2.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.2.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.2.2.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.9.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.9.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.9.10.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.2-tests.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.10.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.10.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.10.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-submarine-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.2.2.jar\n",
      "STARTUP_MSG:   build = Unknown -r 7a3bc90b05f257c8ace2f76d74264906f0f7a932; compiled by 'hexiaoqiao' on 2021-01-03T09:26Z\n",
      "STARTUP_MSG:   java = 1.8.0_292\n",
      "************************************************************/\n",
      "2021-06-27 19:01:15,939 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "2021-06-27 19:01:16,036 INFO namenode.NameNode: createNameNode [-format, -force, -nonInteractive]\n",
      "2021-06-27 19:01:16,418 INFO common.Util: Assuming 'file' scheme for path /opt/hadoop/data/nameNode in configuration.\n",
      "2021-06-27 19:01:16,418 INFO common.Util: Assuming 'file' scheme for path /opt/hadoop/data/nameNode in configuration.\n",
      "Formatting using clusterid: CID-26d719fe-7e8c-454c-ae01-29c813cabbd0\n",
      "2021-06-27 19:01:16,447 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "2021-06-27 19:01:16,476 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "2021-06-27 19:01:16,477 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "2021-06-27 19:01:16,478 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "2021-06-27 19:01:16,482 INFO namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)\n",
      "2021-06-27 19:01:16,482 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "2021-06-27 19:01:16,482 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
      "2021-06-27 19:01:16,483 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "2021-06-27 19:01:16,537 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "2021-06-27 19:01:16,542 ERROR blockmanagement.DatanodeManager: error reading hosts files: \n",
      "java.io.FileNotFoundException: /opt/hadoop/etc/hadoop/dfs.exclude (No such file or directory)\n",
      "\tat java.io.FileInputStream.open0(Native Method)\n",
      "\tat java.io.FileInputStream.open(FileInputStream.java:195)\n",
      "\tat java.io.FileInputStream.<init>(FileInputStream.java:138)\n",
      "\tat org.apache.hadoop.util.HostsFileReader.readFileToSet(HostsFileReader.java:79)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.readFile(HostFileManager.java:79)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.refresh(HostFileManager.java:158)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.refresh(HostFileManager.java:71)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:262)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:465)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:831)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:767)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1208)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1783)\n",
      "2021-06-27 19:01:16,549 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "2021-06-27 19:01:16,549 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "2021-06-27 19:01:16,553 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "2021-06-27 19:01:16,554 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Jun 27 19:01:16\n",
      "2021-06-27 19:01:16,555 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "2021-06-27 19:01:16,555 INFO util.GSet: VM type       = 64-bit\n",
      "2021-06-27 19:01:16,557 INFO util.GSet: 2.0% max memory 910.5 MB = 18.2 MB\n",
      "2021-06-27 19:01:16,557 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "2021-06-27 19:01:16,564 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
      "2021-06-27 19:01:16,564 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
      "2021-06-27 19:01:16,570 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n",
      "2021-06-27 19:01:16,570 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "2021-06-27 19:01:16,570 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "2021-06-27 19:01:16,570 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "2021-06-27 19:01:16,571 INFO blockmanagement.BlockManager: defaultReplication         = 2\n",
      "2021-06-27 19:01:16,571 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "2021-06-27 19:01:16,571 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "2021-06-27 19:01:16,571 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "2021-06-27 19:01:16,571 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
      "2021-06-27 19:01:16,571 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "2021-06-27 19:01:16,571 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "2021-06-27 19:01:16,595 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
      "2021-06-27 19:01:16,595 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
      "2021-06-27 19:01:16,595 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
      "2021-06-27 19:01:16,596 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
      "2021-06-27 19:01:16,608 INFO util.GSet: Computing capacity for map INodeMap\n",
      "2021-06-27 19:01:16,608 INFO util.GSet: VM type       = 64-bit\n",
      "2021-06-27 19:01:16,609 INFO util.GSet: 1.0% max memory 910.5 MB = 9.1 MB\n",
      "2021-06-27 19:01:16,609 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "2021-06-27 19:01:16,609 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "2021-06-27 19:01:16,609 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
      "2021-06-27 19:01:16,609 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "2021-06-27 19:01:16,609 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "2021-06-27 19:01:16,615 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
      "2021-06-27 19:01:16,616 INFO snapshot.SnapshotManager: SkipList is disabled\n",
      "2021-06-27 19:01:16,621 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "2021-06-27 19:01:16,621 INFO util.GSet: VM type       = 64-bit\n",
      "2021-06-27 19:01:16,621 INFO util.GSet: 0.25% max memory 910.5 MB = 2.3 MB\n",
      "2021-06-27 19:01:16,621 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
      "2021-06-27 19:01:16,655 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "2021-06-27 19:01:16,655 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "2021-06-27 19:01:16,655 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "2021-06-27 19:01:16,660 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "2021-06-27 19:01:16,660 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "2021-06-27 19:01:16,662 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "2021-06-27 19:01:16,662 INFO util.GSet: VM type       = 64-bit\n",
      "2021-06-27 19:01:16,662 INFO util.GSet: 0.029999999329447746% max memory 910.5 MB = 279.7 KB\n",
      "2021-06-27 19:01:16,662 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
      "2021-06-27 19:01:16,688 INFO namenode.FSImage: Allocated new BlockPoolId: BP-358313605-172.17.0.2-1624820476678\n",
      "2021-06-27 19:01:16,717 INFO common.Storage: Storage directory /opt/hadoop-3.2.2/data/nameNode has been successfully formatted.\n",
      "2021-06-27 19:01:16,746 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/hadoop-3.2.2/data/nameNode/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "2021-06-27 19:01:16,849 INFO namenode.FSImageFormatProtobuf: Image file /opt/hadoop-3.2.2/data/nameNode/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .\n",
      "2021-06-27 19:01:16,868 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "2021-06-27 19:01:16,871 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
      "2021-06-27 19:01:16,872 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at hadoop/172.17.0.2\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs namenode -format -force -nonInteractive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Hadoop daemons\n",
    "\n",
    "- manual execution: ```hdfs --daemon start (namenode|datanode)``` and ```yarn --daemon start (resourcemanager|nodemanager)```\n",
    "- auxilliary scripts to run all processes on the cluster: start-dfs.sh (HDFS) and start-yarn.sh (YARN)\n",
    "- some services still need to be executed manually (timelineserver, historyserver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "Starting datanodes\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop3: datanode is running as process 131.  Stop it first.\n",
      "pdsh@hadoop: hadoop3: ssh exited with exit code 1\n",
      "hadoop2: datanode is running as process 131.  Stop it first.\n",
      "pdsh@hadoop: hadoop2: ssh exited with exit code 1\n",
      "hadoop1: datanode is running as process 131.  Stop it first.\n",
      "pdsh@hadoop: hadoop1: ssh exited with exit code 1\n",
      "Starting secondary namenodes [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "hadoop: secondarynamenode is running as process 518.  Stop it first.\n",
      "pdsh@hadoop: hadoop: ssh exited with exit code 1\n",
      "Starting resourcemanager\n",
      "resourcemanager is running as process 780.  Stop it first.\n",
      "Starting nodemanagers\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop1: nodemanager is running as process 239.  Stop it first.\n",
      "pdsh@hadoop: hadoop1: ssh exited with exit code 1\n",
      "hadoop3: nodemanager is running as process 239.  Stop it first.\n",
      "pdsh@hadoop: hadoop3: ssh exited with exit code 1\n",
      "hadoop2: nodemanager is running as process 239.  Stop it first.\n",
      "pdsh@hadoop: hadoop2: ssh exited with exit code 1\n",
      "timelineserver is running as process 1148.  Stop it first.\n",
      "historyserver is running as process 1218.  Stop it first.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# HDFS\n",
    "start-dfs.sh\n",
    "\n",
    "# YARN\n",
    "start-yarn.sh\n",
    "\n",
    "# timelineserver\n",
    "yarn --daemon start timelineserver\n",
    "\n",
    "# historyserver\n",
    "mapred --daemon start historyserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      "1218 JobHistoryServer\n",
      "518 SecondaryNameNode\n",
      "780 ResourceManager\n",
      "1148 ApplicationHistoryServer\n",
      "2253 Jps\n",
      "1599 NameNode\n",
      "hadoop1\n",
      "131 DataNode\n",
      "538 Jps\n",
      "239 NodeManager\n",
      "hadoop2\n",
      "131 DataNode\n",
      "539 Jps\n",
      "239 NodeManager\n",
      "hadoop3\n",
      "131 DataNode\n",
      "538 Jps\n",
      "239 NodeManager\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Listing all processes\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3; do\n",
    "    echo $HOST\n",
    "    docker exec $HOST jps\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HDFS directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp': File exists\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/hadoop\n",
    "hdfs dfs -chown hadoop:hadoop /user/hadoop\n",
    "hdfs dfs -mkdir /tmp\n",
    "hdfs dfs -chmod 777 /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and run Jupyter on master node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2021-06-27 19:41:49.287 ServerApp] jupyterlab | extension was successfully linked.\n",
      "[W 2021-06-27 19:41:49.306 ServerApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.\n",
      "[I 2021-06-27 19:41:49.308 LabApp] JupyterLab extension loaded from /home/hadoop/.local/lib/python3.6/site-packages/jupyterlab\n",
      "[I 2021-06-27 19:41:49.308 LabApp] JupyterLab application directory is /home/hadoop/.local/share/jupyter/lab\n",
      "[I 2021-06-27 19:41:49.313 ServerApp] jupyterlab | extension was successfully loaded.\n",
      "[I 2021-06-27 19:41:49.313 ServerApp] Serving notebooks from local directory: /\n",
      "[I 2021-06-27 19:41:49.313 ServerApp] Jupyter Server 1.9.0 is running at:\n",
      "[I 2021-06-27 19:41:49.313 ServerApp] http://172.17.0.2:8080/lab\n",
      "[I 2021-06-27 19:41:49.313 ServerApp]  or http://127.0.0.1:8080/lab\n",
      "[I 2021-06-27 19:41:49.313 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# pip3 -q install notebook\n",
    "pip3 -q install jupyterlab\n",
    "\n",
    "IP=$(ifconfig eth0 | grep inet | awk '{ print $2 }')\n",
    "\n",
    "cd /opt\n",
    "\n",
    "export SHELL=/bin/bash\n",
    "# nohup /home/hadoop/.local/bin/jupyter-notebook --ip=$IP --port=8080 --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.notebook_dir='/' --no-browser &\n",
    "nohup /home/hadoop/.local/bin/jupyter-lab --ip=$IP --port=8080 --notebook-dir='/' --ServerApp.token='' --ServerApp.password='' --no-browser &\n",
    "\n",
    "echo $! > /tmp/jupyter.pid\n",
    "\n",
    "# To kill\n",
    "# kill $(cat /tmp/jupyter.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access web interfaces\n",
    "\n",
    "- Jupyterlab\n",
    "    - http://localhost:8080\n",
    "- Master - hadoop\n",
    "    - Resource Manager: http://localhost:8088\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "    - MapReduce Job History: http://localhost:19888\n",
    "    - Timeline Service: http://localhost:8188\n",
    "- Workers\n",
    "    - hadoop1\n",
    "        - NodeManager: http://localhost:8042\n",
    "        - DataNode: http://localhost:9864\n",
    "    - hadoop2\n",
    "        - NodeManager: http://localhost:8043\n",
    "        - DataNode: http://localhost:9865\n",
    "    - hadoop3\n",
    "        - NodeManager: http://localhost:8044\n",
    "        - DataNode: http://localhost:9866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run mapreduce Pi example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Maps  = 6\n",
      "Samples per Map = 10000\n",
      "Wrote input for Map #0\n",
      "Wrote input for Map #1\n",
      "Wrote input for Map #2\n",
      "Wrote input for Map #3\n",
      "Wrote input for Map #4\n",
      "Wrote input for Map #5\n",
      "Starting Job\n",
      "2021-06-26 02:24:45,818 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-06-26 02:24:45,958 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-06-26 02:24:46,095 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1624673771620_0001\n",
      "2021-06-26 02:24:46,246 INFO input.FileInputFormat: Total input files to process : 6\n",
      "2021-06-26 02:24:46,375 INFO mapreduce.JobSubmitter: number of splits:6\n",
      "2021-06-26 02:24:46,534 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1624673771620_0001\n",
      "2021-06-26 02:24:46,535 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-06-26 02:24:46,690 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-06-26 02:24:46,691 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-06-26 02:24:47,088 INFO impl.YarnClientImpl: Submitted application application_1624673771620_0001\n",
      "2021-06-26 02:24:47,123 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1624673771620_0001/\n",
      "2021-06-26 02:24:47,124 INFO mapreduce.Job: Running job: job_1624673771620_0001\n",
      "2021-06-26 02:24:53,268 INFO mapreduce.Job: Job job_1624673771620_0001 running in uber mode : false\n",
      "2021-06-26 02:24:53,269 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-06-26 02:25:01,339 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-06-26 02:25:05,366 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-06-26 02:25:05,380 INFO mapreduce.Job: Job job_1624673771620_0001 completed successfully\n",
      "2021-06-26 02:25:05,472 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=138\n",
      "\t\tFILE: Number of bytes written=1647351\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1578\n",
      "\t\tHDFS: Number of bytes written=215\n",
      "\t\tHDFS: Number of read operations=29\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=6\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=65888\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4194\n",
      "\t\tTotal time spent by all map tasks (ms)=32944\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2097\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=32944\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2097\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8433664\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=536832\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=12\n",
      "\t\tMap output bytes=108\n",
      "\t\tMap output materialized bytes=168\n",
      "\t\tInput split bytes=870\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=168\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=24\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=1364\n",
      "\t\tCPU time spent (ms)=3930\n",
      "\t\tPhysical memory (bytes) snapshot=1854074880\n",
      "\t\tVirtual memory (bytes) snapshot=13578498048\n",
      "\t\tTotal committed heap usage (bytes)=1280311296\n",
      "\t\tPeak Map Physical memory (bytes)=281788416\n",
      "\t\tPeak Map Virtual memory (bytes)=1942220800\n",
      "\t\tPeak Reduce Physical memory (bytes)=182161408\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1948155904\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=708\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=97\n",
      "Job Finished in 19.84 seconds\n",
      "Estimated value of Pi is 3.14200000000000000000\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.2.jar pi 6 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHUTDOWN PROCEDURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "kill $(cat /tmp/jupyter.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Hadoop daemons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "stop-dfs.sh\n",
    "stop-yarn.sh\n",
    "yarn --daemon stop timelineserver\n",
    "mapred --daemon stop historyserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Docker containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3; do\n",
    "    docker stop $HOST\n",
    "done\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
